{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIJjlEdZigV1",
        "outputId": "d53a8877-d550-4452-8a4d-2ffde5eade66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMPRZMysij1_"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "!pip install keras_preprocessing\n",
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TwYmLcU8iHob"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout, Bidirectional, Input, Concatenate, concatenate\n",
        "from keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from keras.models import Model\n",
        "\n",
        "import os.path\n",
        "import wget\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4bXyVUvCiLLY"
      },
      "outputs": [],
      "source": [
        "def assign_class(label):\n",
        "    if label == \"OR\":\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Sr1lsLTs-V8u"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "def evaluate_model(y_prob, y_test):\n",
        "    y_pred = np.round(y_prob)\n",
        "    y_true = y_test\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "\n",
        "    metrics = {'accuracy': accuracy,\n",
        "               'precision': precision,\n",
        "               'recall': recall,\n",
        "               'f1': f1,\n",
        "               'confusion_matrix': cm,\n",
        "               'roc_auc': roc_auc}\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaqZhuQCQd8E"
      },
      "outputs": [],
      "source": [
        "# import and extract information \n",
        "\n",
        "ori_train_data = pd.read_csv('/content/drive/MyDrive/swm-data/ori_data_final_train.csv')\n",
        "ori_valid_data = pd.read_csv('/content/drive/MyDrive/swm-data/ori_data_final_val.csv')\n",
        "ori_test_data = pd.read_csv('/content/drive/MyDrive/swm-data/ori_data_final_test.csv')\n",
        "\n",
        "ori_train_data['label_final'] = ori_train_data['label'].apply(assign_class)\n",
        "ori_valid_data['label_final'] = ori_valid_data['label'].apply(assign_class)\n",
        "ori_test_data['label_final'] = ori_test_data['label'].apply(assign_class)\n",
        "\n",
        "ori_train_data.category = pd.factorize(ori_train_data.category)[0]\n",
        "ori_test_data.category = pd.factorize(ori_test_data.category)[0]\n",
        "ori_valid_data.category = pd.factorize(ori_valid_data.category)[0]\n",
        "category_indices = pd.factorize(ori_train_data.category)[1]\n",
        "\n",
        "x_train_review = ori_train_data.text_final\n",
        "x_train_category = ori_train_data.category\n",
        "x_train_word_category = ori_train_data.word_count_categories\n",
        "\n",
        "x_test_review = ori_test_data.text_final\n",
        "x_test_category = ori_test_data.category\n",
        "x_test_word_category = ori_test_data.word_count_categories\n",
        "\n",
        "x_val_review = ori_valid_data.text_final\n",
        "x_val_category = ori_valid_data.category\n",
        "x_val_word_category = ori_valid_data.word_count_categories\n",
        "\n",
        "y_train = ori_train_data.label_final\n",
        "y_test = ori_test_data.label_final\n",
        "y_val = ori_valid_data.label_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNasOyTOQd8F"
      },
      "outputs": [],
      "source": [
        "# create vocab \n",
        "\n",
        "vocab_size = 20000\n",
        "oov_token = \"\"\n",
        "max_length = 200\n",
        "padding_type = \"post\"\n",
        "trunc_type = \"post\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(x_train_review) \n",
        "\n",
        "x_train_review_pad = pad_sequences(tokenizer.texts_to_sequences(\n",
        "    x_train_review), maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "x_test_review_pad = pad_sequences(tokenizer.texts_to_sequences(\n",
        "    x_test_review), maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "x_val_review_pad = pad_sequences(tokenizer.texts_to_sequences(\n",
        "    x_val_review), maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2m7EdvJQd8F",
        "outputId": "e000eb48-c437-44f8-915c-22ae466347ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Converted 30442 words (7491 misses)\n"
          ]
        }
      ],
      "source": [
        "# if not os.path.isfile('/content/drive/MyDrive/swm-data/glove.6B.@00d.txt'):\n",
        "#     # large file, might take a while to download :)\n",
        "#     url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "#     print('Downloading Pre-trained Word Embeddings')\n",
        "#     wget.download(url)\n",
        "#     print('Download Completed!\\nUnzipping...')\n",
        "#     shutil.unpack_archive('glove.6B.zip')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(\"/content/drive/MyDrive/swm-data/glove.6B.200d.txt\") as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "num_tokens = len(tokenizer.word_index) + 2\n",
        "embedding_dim = 200\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9oK--ALQd8F"
      },
      "outputs": [],
      "source": [
        "# create embedding layer \n",
        "import keras\n",
        "from keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcWIApvVQd8G"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "\n",
        "    review_branch_input = Input(shape=(200,), name='review_input')\n",
        "    review_branch = embedding_layer(review_branch_input)\n",
        "    review_branch = Dropout(0.2)(review_branch)\n",
        "    review_branch = Bidirectional(\n",
        "        LSTM(64, dropout=0.2, recurrent_dropout=0))(review_branch)\n",
        "    review_branch = Dense(64, activation='relu')(review_branch)\n",
        "    review_branch_output = Dense(16, activation='relu')(review_branch)\n",
        "\n",
        "    category_branch_input = Input(\n",
        "        shape=(1,), name='category_input')\n",
        "    category_branch = Dense(\n",
        "        8, activation='relu')(category_branch_input)\n",
        "    category_branch = Dropout(0.2)(category_branch)\n",
        "    category_branch_output = Dense(\n",
        "        16, activation='relu')(category_branch)\n",
        "\n",
        "    word_category_branch_input = Input(\n",
        "        shape=(1,), name='word_category_input')\n",
        "    word_category_branch = Dense(\n",
        "        8, activation='relu')(word_category_branch_input)\n",
        "    word_category_branch = Dropout(0.2)(word_category_branch)\n",
        "    word_category_branch_output = Dense(\n",
        "        16, activation='relu')(word_category_branch)\n",
        "\n",
        "    concat = concatenate([review_branch_output,\n",
        "                        category_branch_output, word_category_branch_output], name='Concatenate')\n",
        "\n",
        "    final_output = Dense(8, activation='relu')(concat)\n",
        "    final_output = Dense(1, activation='sigmoid')(final_output)\n",
        "\n",
        "    model = Model(inputs=[review_branch_input,\n",
        "                category_branch_input, word_category_branch_input], outputs=final_output, name='Final_output')\n",
        "\n",
        "    hp_learning_rate = hp.Float(\n",
        "        'learning_rate',\n",
        "        min_value=0.001,\n",
        "        max_value=0.005,\n",
        "        sampling='LOG',\n",
        "        default=1e-3\n",
        "    )\n",
        "\n",
        "    # clip value to avoid the gradient exploding\n",
        "    optimzer = Adam(clipvalue=0.5, learning_rate=hp_learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimzer,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9sq-wZkQd8G"
      },
      "outputs": [],
      "source": [
        "tuner = kt.BayesianOptimization(build_model,\n",
        "                    objective='val_acc',\n",
        "                    max_trials=5,\n",
        "                    directory='adl4nlp1',\n",
        "                    project_name='text_classification_bo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMmbDdgZQd8G",
        "outputId": "09ce9a0f-c770-41e4-f52a-ffda1df4ad08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 1\n",
            "learning_rate (Float)\n",
            "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.005, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzY96N-xQd8G",
        "outputId": "741564e2-1342-4dd4-b1fe-25d253ff3694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 5 Complete [00h 04m 43s]\n",
            "val_acc: 0.4996289908885956\n",
            "\n",
            "Best val_acc So Far: 0.9379174113273621\n",
            "Total elapsed time: 00h 36m 48s\n"
          ]
        }
      ],
      "source": [
        "tuner.search([x_train_review_pad,x_train_category,x_train_word_category], y_train, epochs=5, \n",
        "         validation_data=([x_val_review_pad,x_val_category,x_val_word_category], y_val),\n",
        "         callbacks=[stop_early,tf.keras.callbacks.TensorBoard(\"/tmp/tb_logs\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JslE79IzQd8G",
        "outputId": "edb7d4f3-9a7c-44c7-83e9-4083f5e14472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in adl4nlp1/text_classification_bo\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_acc\", direction=\"max\")\n",
            "\n",
            "Trial 0 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.0016248751135807084\n",
            "Score: 0.9379174113273621\n",
            "\n",
            "Trial 2 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.004051230715430242\n",
            "Score: 0.9065050482749939\n",
            "\n",
            "Trial 3 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.003794776493389722\n",
            "Score: 0.9008162021636963\n",
            "\n",
            "Trial 1 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.0025748641712652173\n",
            "Score: 0.5006183385848999\n",
            "\n",
            "Trial 4 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.0012272957304917229\n",
            "Score: 0.4996289908885956\n"
          ]
        }
      ],
      "source": [
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMMZ6AuPQd8H"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models(num_models=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNz0FnSzQd8H"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJuOS2YCQd8I",
        "outputId": "832f01f7-58ec-4da9-82fe-c905eabbf875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "127/127 - 4s - loss: 0.1618 - acc: 0.9372 - 4s/epoch - 35ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.16179168224334717, 0.9371908903121948]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluate and save model \n",
        "\n",
        "best_model[0].evaluate([x_test_review_pad, x_test_category, x_test_word_category], y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O74tbwmEQd8I"
      },
      "outputs": [],
      "source": [
        "best_model[0].save('/content/drive/MyDrive/swm-data/bilstm_old_t.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMv68M55Qd8I",
        "outputId": "144f1781-0e4c-4347-c658-2d717a35736a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ],
      "source": [
        "# Reload the model and verify the accuracy.\n",
        "\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/swm-data/bilstm_old_t.h5')\n",
        "model.evaluate([x_test_review_pad, x_test_category, x_test_word_category], y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DExPIG21Qd8I",
        "outputId": "e8e68758-1c01-43d2-b272-86191af9deca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  4/127 [..............................] - ETA: 3s"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "127/127 [==============================] - 5s 40ms/step\n",
            "Test accuracy: 0.9371909000989119\n",
            "Test precision: 0.9233381853469189\n",
            "Test recall: 0.951975987993997\n",
            "Test F1 score: 0.9374384236453202\n",
            "Test ROC-AUC score: 0.9854176476991552\n",
            "Test confusion matrix:\n",
            " [[1887  158]\n",
            " [  96 1903]]\n"
          ]
        }
      ],
      "source": [
        "# Get predictions \n",
        "y_prob = model.predict([x_test_review_pad, x_test_category, x_test_word_category])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_metrics = evaluate_model(y_prob, y_test)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Test accuracy:', test_metrics['accuracy'])\n",
        "print('Test precision:', test_metrics['precision'])\n",
        "print('Test recall:', test_metrics['recall'])\n",
        "print('Test F1 score:', test_metrics['f1'])\n",
        "print('Test ROC-AUC score:', test_metrics['roc_auc'])\n",
        "print('Test confusion matrix:\\n', test_metrics['confusion_matrix'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8bMJMmJ9Qd8D",
        "6RYUv70FUeEy",
        "j3B1DAhsUPlH",
        "06eaWzzjT9Fh",
        "Ln7Iqgl3MnxE"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
