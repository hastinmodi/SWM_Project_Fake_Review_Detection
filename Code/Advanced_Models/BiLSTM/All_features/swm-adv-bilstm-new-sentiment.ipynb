{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIJjlEdZigV1",
        "outputId": "d53a8877-d550-4452-8a4d-2ffde5eade66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMPRZMysij1_"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "!pip install keras_preprocessing\n",
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TwYmLcU8iHob"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout, Bidirectional, Input, Concatenate, concatenate\n",
        "from keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from keras.models import Model\n",
        "\n",
        "import os.path\n",
        "import wget\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4bXyVUvCiLLY"
      },
      "outputs": [],
      "source": [
        "def assign_class(label):\n",
        "    if label == \"OR\":\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Sr1lsLTs-V8u"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "def evaluate_model(y_prob, y_test):\n",
        "    y_pred = np.round(y_prob)\n",
        "    y_true = y_test\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "\n",
        "    metrics = {'accuracy': accuracy,\n",
        "               'precision': precision,\n",
        "               'recall': recall,\n",
        "               'f1': f1,\n",
        "               'confusion_matrix': cm,\n",
        "               'roc_auc': roc_auc}\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld7UMPmNUeE0"
      },
      "outputs": [],
      "source": [
        "# import and extract information \n",
        "\n",
        "new_train_data = pd.read_csv('/content/drive/MyDrive/swm-data/new_data_final_train.csv')\n",
        "new_valid_data = pd.read_csv('/content/drive/MyDrive/swm-data/new_data_final_val.csv')\n",
        "new_test_data = pd.read_csv('/content/drive/MyDrive/swm-data/new_data_final_test.csv')\n",
        "\n",
        "new_train_data['label_final'] = new_train_data['label'].apply(assign_class)\n",
        "new_valid_data['label_final'] = new_valid_data['label'].apply(assign_class)\n",
        "new_test_data['label_final'] = new_test_data['label'].apply(assign_class)\n",
        "\n",
        "new_train_data.category = pd.factorize(new_train_data.category)[0]\n",
        "new_test_data.category = pd.factorize(new_test_data.category)[0]\n",
        "new_valid_data.category = pd.factorize(new_valid_data.category)[0]\n",
        "category_indices = pd.factorize(new_train_data.category)[1]\n",
        "\n",
        "x_train_review = new_train_data.text_final\n",
        "x_train_sentiment = new_train_data.sentiment\n",
        "x_train_category = new_train_data.category\n",
        "x_train_word_category = new_train_data.word_count_categories\n",
        "\n",
        "x_test_review = new_test_data.text_final\n",
        "x_test_sentiment = new_test_data.sentiment\n",
        "x_test_category = new_test_data.category\n",
        "x_test_word_category = new_test_data.word_count_categories\n",
        "\n",
        "\n",
        "x_val_review = new_valid_data.text_final\n",
        "x_val_sentiment = new_valid_data.sentiment\n",
        "x_val_category = new_valid_data.category\n",
        "x_val_word_category = new_valid_data.word_count_categories\n",
        "\n",
        "\n",
        "y_train = new_train_data.label_final\n",
        "y_test = new_test_data.label_final\n",
        "y_val = new_valid_data.label_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GhDdDRNUeE1"
      },
      "outputs": [],
      "source": [
        "# create vocab \n",
        "\n",
        "vocab_size = 20000\n",
        "oov_token = \"\"\n",
        "max_length = 200\n",
        "padding_type = \"post\"\n",
        "trunc_type = \"post\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(x_train_review) \n",
        "\n",
        "x_train_review_pad = pad_sequences(tokenizer.texts_to_sequences(\n",
        "    x_train_review), maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "x_test_review_pad = pad_sequences(tokenizer.texts_to_sequences(\n",
        "    x_test_review), maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "x_val_review_pad = pad_sequences(tokenizer.texts_to_sequences(\n",
        "    x_val_review), maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoWTn0RtUeE2",
        "outputId": "b2704d48-cfe4-4edb-f77b-a3f9a17a1d02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Converted 32281 words (8707 misses)\n"
          ]
        }
      ],
      "source": [
        "# if not os.path.isfile('/content/drive/MyDrive/swm-data/glove.6B.@00d.txt'):\n",
        "#     # large file, might take a while to download :)\n",
        "#     url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "#     print('Downloading Pre-trained Word Embeddings')\n",
        "#     wget.download(url)\n",
        "#     print('Download Completed!\\nUnzipping...')\n",
        "#     shutil.unpack_archive('glove.6B.zip')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(\"/content/drive/MyDrive/swm-data/glove.6B.200d.txt\") as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "num_tokens = len(tokenizer.word_index) + 2\n",
        "embedding_dim = 200\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PSwc5SCUeE2"
      },
      "outputs": [],
      "source": [
        "# create embedding layer \n",
        "import keras\n",
        "from keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YEXsoIoUeE2"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "\n",
        "    review_branch_input = Input(shape=(200,), name='review_input')\n",
        "    review_branch = embedding_layer(review_branch_input)\n",
        "    review_branch = Dropout(0.2)(review_branch)\n",
        "    review_branch = Bidirectional(\n",
        "        LSTM(64, dropout=0.2, recurrent_dropout=0))(review_branch)\n",
        "    review_branch = Dense(64, activation='relu')(review_branch)\n",
        "    review_branch_output = Dense(16, activation='relu')(review_branch)\n",
        "\n",
        "    sentiment_branch_input = Input(shape=(1,), name='sentiment_input')\n",
        "    sentiment_branch = Dense(8, activation='relu')(sentiment_branch_input)\n",
        "    sentiment_branch = Dropout(0.2)(sentiment_branch)\n",
        "    sentiment_branch_output = Dense(16, activation='relu')(sentiment_branch)\n",
        "\n",
        "    category_branch_input = Input(\n",
        "        shape=(1,), name='category_input')\n",
        "    category_branch = Dense(\n",
        "        8, activation='relu')(category_branch_input)\n",
        "    category_branch = Dropout(0.2)(category_branch)\n",
        "    category_branch_output = Dense(\n",
        "        16, activation='relu')(category_branch)\n",
        "\n",
        "    word_category_branch_input = Input(\n",
        "        shape=(1,), name='word_category_input')\n",
        "    word_category_branch = Dense(\n",
        "        8, activation='relu')(word_category_branch_input)\n",
        "    word_category_branch = Dropout(0.2)(word_category_branch)\n",
        "    word_category_branch_output = Dense(\n",
        "        16, activation='relu')(word_category_branch)\n",
        "\n",
        "    concat = concatenate([review_branch_output, sentiment_branch_output,\n",
        "                        category_branch_output, word_category_branch_output], name='Concatenate')\n",
        "\n",
        "    final_output = Dense(8, activation='relu')(concat)\n",
        "    final_output = Dense(1, activation='sigmoid')(final_output)\n",
        "\n",
        "    model = Model(inputs=[review_branch_input, sentiment_branch_input,\n",
        "                category_branch_input, word_category_branch_input], outputs=final_output, name='Final_output')\n",
        "\n",
        "    hp_learning_rate = hp.Float(\n",
        "        'learning_rate',\n",
        "        min_value=0.001,\n",
        "        max_value=0.005,\n",
        "        sampling='LOG',\n",
        "        default=1e-3\n",
        "    )\n",
        "\n",
        "    # clip value to avoid the gradient exploding\n",
        "    optimzer = Adam(clipvalue=0.5, learning_rate=hp_learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimzer,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gORGz8Z-UeE3"
      },
      "outputs": [],
      "source": [
        "tuner = kt.BayesianOptimization(build_model,\n",
        "                    objective='val_acc',\n",
        "                    max_trials=5,\n",
        "                    directory='adl4nlpnew1',\n",
        "                    project_name='text_classification_bo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W6Uf8i3UeE3",
        "outputId": "db7673ce-3686-4733-b61a-c5c9284f8ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 1\n",
            "learning_rate (Float)\n",
            "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.005, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU05IhPhUeE3",
        "outputId": "0b7c0f01-94b8-4c4f-e078-f8eaf0d29aca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 5 Complete [00h 01m 44s]\n",
            "val_acc: 0.725698709487915\n",
            "\n",
            "Best val_acc So Far: 0.7615631818771362\n",
            "Total elapsed time: 00h 10m 06s\n"
          ]
        }
      ],
      "source": [
        "tuner.search([x_train_review_pad,x_train_sentiment,x_train_category,x_train_word_category], y_train, epochs=5, \n",
        "         validation_data=([x_val_review_pad,x_val_sentiment,x_val_category,x_val_word_category], y_val),\n",
        "         callbacks=[stop_early,tf.keras.callbacks.TensorBoard(\"/tmp/tb_logs\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3839-8CmUeE4",
        "outputId": "de4c351b-57ad-47d9-e6be-8a04698a413d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in adl4nlpnew1/text_classification_bo\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_acc\", direction=\"max\")\n",
            "\n",
            "Trial 1 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.001614877243195852\n",
            "Score: 0.7615631818771362\n",
            "\n",
            "Trial 2 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.0032593607643704955\n",
            "Score: 0.7425179481506348\n",
            "\n",
            "Trial 0 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.00169837472355136\n",
            "Score: 0.742023229598999\n",
            "\n",
            "Trial 3 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.0015440468833853918\n",
            "Score: 0.7415285706520081\n",
            "\n",
            "Trial 4 summary\n",
            "Hyperparameters:\n",
            "learning_rate: 0.004858413593573578\n",
            "Score: 0.725698709487915\n"
          ]
        }
      ],
      "source": [
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE75P7KUUeE5"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models(num_models=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8lsaAoFUeE6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoDoBB1RUeE6",
        "outputId": "1d9ea6f3-85ca-47a9-8475-ff9eb25de21c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "127/127 - 5s - loss: 0.4930 - acc: 0.7483 - 5s/epoch - 37ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.4930415451526642, 0.7482690215110779]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluate and save model \n",
        "\n",
        "best_model[0].evaluate([x_test_review_pad, x_test_sentiment, x_test_category, x_test_word_category], y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAOtQDcIUeE6"
      },
      "outputs": [],
      "source": [
        "best_model[0].save('/content/drive/MyDrive/swm-data/bilstm_new_ts.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4PDUgHqUeE6"
      },
      "outputs": [],
      "source": [
        "# Reload the model and verify the accuracy.\n",
        "\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/swm-data/bilstm_new_ts.h5')\n",
        "model.evaluate([x_test_review_pad, x_test_sentiment, x_test_category, x_test_word_category], y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp1v87pPAL4m",
        "outputId": "e3d7e585-4278-442c-a977-c6475e1131f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  3/127 [..............................] - ETA: 3s"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "127/127 [==============================] - 4s 34ms/step\n",
            "Test accuracy: 0.7457962413452027\n",
            "Test precision: 0.7261790182868142\n",
            "Test recall: 0.7667682926829268\n",
            "Test F1 score: 0.745921898171033\n",
            "Test ROC-AUC score: 0.8298476490906528\n",
            "Test confusion matrix:\n",
            " [[1507  569]\n",
            " [ 459 1509]]\n"
          ]
        }
      ],
      "source": [
        "# Get predictions \n",
        "y_prob = model.predict([x_test_review_pad, x_test_sentiment, x_test_category, x_test_word_category])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_metrics = evaluate_model(y_prob, y_test)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Test accuracy:', test_metrics['accuracy'])\n",
        "print('Test precision:', test_metrics['precision'])\n",
        "print('Test recall:', test_metrics['recall'])\n",
        "print('Test F1 score:', test_metrics['f1'])\n",
        "print('Test ROC-AUC score:', test_metrics['roc_auc'])\n",
        "print('Test confusion matrix:\\n', test_metrics['confusion_matrix'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8bMJMmJ9Qd8D",
        "6RYUv70FUeEy",
        "j3B1DAhsUPlH",
        "06eaWzzjT9Fh",
        "Ln7Iqgl3MnxE"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
