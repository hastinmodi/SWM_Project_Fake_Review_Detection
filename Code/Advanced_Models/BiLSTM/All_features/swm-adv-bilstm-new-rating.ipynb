{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIJjlEdZigV1",
        "outputId": "d53a8877-d550-4452-8a4d-2ffde5eade66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMPRZMysij1_"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "!pip install keras_preprocessing\n",
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TwYmLcU8iHob"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout, Bidirectional, Input, Concatenate, concatenate\n",
        "from keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from keras.models import Model\n",
        "\n",
        "import os.path\n",
        "import wget\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4bXyVUvCiLLY"
      },
      "outputs": [],
      "source": [
        "def assign_class(label):\n",
        "    if label == \"OR\":\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Sr1lsLTs-V8u"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "def evaluate_model(y_prob, y_test):\n",
        "    y_pred = np.round(y_prob)\n",
        "    y_true = y_test\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "\n",
        "    metrics = {'accuracy': accuracy,\n",
        "               'precision': precision,\n",
        "               'recall': recall,\n",
        "               'f1': f1,\n",
        "               'confusion_matrix': cm,\n",
        "               'roc_auc': roc_auc}\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofEQf78fUPlK"
      },
      "outputs": [],
      "source": [
        "# import and extract information \n",
        "\n",
        "new_train_data = pd.read_csv('/content/drive/MyDrive/swm-data/new_data_final_train.csv')\n",
        "new_valid_data = pd.read_csv('/content/drive/MyDrive/swm-data/new_data_final_val.csv')\n",
        "new_test_data = pd.read_csv('/content/drive/MyDrive/swm-data/new_data_final_test.csv')\n",
        "\n",
        "new_train_data['label_final'] = new_train_data['label'].apply(assign_class)\n",
        "new_valid_data['label_final'] = new_valid_data['label'].apply(assign_class)\n",
        "new_test_data['label_final'] = new_test_data['label'].apply(assign_class)\n",
        "\n",
        "new_train_data.category = pd.factorize(new_train_data.category)[0]\n",
        "new_test_data.category = pd.factorize(new_test_data.category)[0]\n",
        "new_valid_data.category = pd.factorize(new_valid_data.category)[0]\n",
        "category_indices = pd.factorize(new_train_data.category)[1]\n",
        "\n",
        "x_train_review = new_train_data.text_final\n",
        "x_train_rating = new_train_data.rating\n",
        "x_train_category = new_train_data.category\n",
        "x_train_word_category = new_train_data.word_count_categories\n",
        "\n",
        "x_test_review = new_test_data.text_final\n",
        "x_test_rating = new_test_data.rating\n",
        "x_test_category = new_test_data.category\n",
        "x_test_word_category = new_test_data.word_count_categories\n",
        "\n",
        "\n",
        "x_val_review = new_valid_data.text_final\n",
        "x_val_rating = new_valid_data.rating\n",
        "x_val_category = new_valid_data.category\n",
        "x_val_word_category = new_valid_data.word_count_categories\n",
        "\n",
        "\n",
        "y_train = new_train_data.label_final\n",
        "y_test = new_test_data.label_final\n",
        "y_val = new_valid_data.label_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcHa07WKUPlL"
      },
      "outputs": [],
      "source": [
        "# create vocab \n",
        "\n",
        "vocab_size = 20000\n",
        "oov_token = \"\"\n",
        "max_length = 200\n",
        "padding_type = \"post\"\n",
        "trunc_type = \"post\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(x_train_review) \n",
        "\n",
        "x_train_review_pad = pad_sequences(tokenizer.texts_to_sequences(\n",
        "    x_train_review), maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "x_test_review_pad = pad_sequences(tokenizer.texts_to_sequences(\n",
        "    x_test_review), maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "x_val_review_pad = pad_sequences(tokenizer.texts_to_sequences(\n",
        "    x_val_review), maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcAPd3i4UPlL",
        "outputId": "36b3afbd-b58f-4094-f25d-048944cc6bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Converted 32281 words (8707 misses)\n"
          ]
        }
      ],
      "source": [
        "# if not os.path.isfile('/content/drive/MyDrive/swm-data/glove.6B.@00d.txt'):\n",
        "#     # large file, might take a while to download :)\n",
        "#     url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "#     print('Downloading Pre-trained Word Embeddings')\n",
        "#     wget.download(url)\n",
        "#     print('Download Completed!\\nUnzipping...')\n",
        "#     shutil.unpack_archive('glove.6B.zip')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(\"/content/drive/MyDrive/swm-data/glove.6B.200d.txt\") as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "num_tokens = len(tokenizer.word_index) + 2\n",
        "embedding_dim = 200\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij2bvIV2UPlM"
      },
      "outputs": [],
      "source": [
        "# create embedding layer \n",
        "import keras\n",
        "from keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpAYIur-UPlM"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "\n",
        "    review_branch_input = Input(shape=(200,), name='review_input')\n",
        "    review_branch = embedding_layer(review_branch_input)\n",
        "    review_branch = Dropout(0.2)(review_branch)\n",
        "    review_branch = Bidirectional(\n",
        "        LSTM(64, dropout=0.2, recurrent_dropout=0))(review_branch)\n",
        "    review_branch = Dense(64, activation='relu')(review_branch)\n",
        "    review_branch_output = Dense(16, activation='relu')(review_branch)\n",
        "\n",
        "    rating_branch_input = Input(shape=(1,), name='rating_input')\n",
        "    rating_branch = Dense(8, activation='relu')(rating_branch_input)\n",
        "    rating_branch = Dropout(0.2)(rating_branch)\n",
        "    rating_branch_output = Dense(16, activation='relu')(rating_branch)\n",
        "\n",
        "    category_branch_input = Input(\n",
        "        shape=(1,), name='category_input')\n",
        "    category_branch = Dense(\n",
        "        8, activation='relu')(category_branch_input)\n",
        "    category_branch = Dropout(0.2)(category_branch)\n",
        "    category_branch_output = Dense(\n",
        "        16, activation='relu')(category_branch)\n",
        "\n",
        "    word_category_branch_input = Input(\n",
        "        shape=(1,), name='word_category_input')\n",
        "    word_category_branch = Dense(\n",
        "        8, activation='relu')(word_category_branch_input)\n",
        "    word_category_branch = Dropout(0.2)(word_category_branch)\n",
        "    word_category_branch_output = Dense(\n",
        "        16, activation='relu')(word_category_branch)\n",
        "\n",
        "    concat = concatenate([review_branch_output, rating_branch_output,\n",
        "                        category_branch_output, word_category_branch_output], name='Concatenate')\n",
        "\n",
        "    final_output = Dense(8, activation='relu')(concat)\n",
        "    final_output = Dense(1, activation='sigmoid')(final_output)\n",
        "\n",
        "    model = Model(inputs=[review_branch_input, rating_branch_input,\n",
        "                category_branch_input, word_category_branch_input], outputs=final_output, name='Final_output')\n",
        "\n",
        "    hp_learning_rate = hp.Float(\n",
        "        'learning_rate',\n",
        "        min_value=0.001,\n",
        "        max_value=0.005,\n",
        "        sampling='LOG',\n",
        "        default=1e-3\n",
        "    )\n",
        "\n",
        "    # clip value to avoid the gradient exploding\n",
        "    optimzer = Adam(clipvalue=0.5, learning_rate=hp_learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimzer,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H40YNaWUPlN"
      },
      "outputs": [],
      "source": [
        "tuner = kt.BayesianOptimization(build_model,\n",
        "                    objective='val_acc',\n",
        "                    max_trials=5,\n",
        "                    directory='adl4nlpnew2',\n",
        "                    project_name='text_classification_bo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9FX41pPUPlN",
        "outputId": "6b25ed6c-4091-415f-aa3a-a8f35924a63e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 1\n",
            "learning_rate (Float)\n",
            "{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.005, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4UhAqupUPlN",
        "outputId": "a9d91e22-d265-463f-e4ad-0d764563bfab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 2 Complete [00h 08m 38s]\n",
            "val_acc: 0.7383131384849548\n",
            "\n",
            "Best val_acc So Far: 0.7494434714317322\n",
            "Total elapsed time: 00h 17m 07s\n",
            "\n",
            "Search: Running Trial #3\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "0.0030188         |0.0012382         |learning_rate\n",
            "\n",
            "Epoch 1/5\n",
            "1011/1011 [==============================] - 99s 98ms/step - loss: 0.6285 - acc: 0.6278 - val_loss: 0.5800 - val_acc: 0.6809\n",
            "Epoch 2/5\n",
            "1011/1011 [==============================] - 102s 101ms/step - loss: 0.5644 - acc: 0.6990 - val_loss: 0.5285 - val_acc: 0.7245\n",
            "Epoch 3/5\n",
            "1011/1011 [==============================] - 96s 95ms/step - loss: 0.5307 - acc: 0.7262 - val_loss: 0.5113 - val_acc: 0.7361\n",
            "Epoch 4/5\n",
            " 881/1011 [=========================>....] - ETA: 11s - loss: 0.5081 - acc: 0.7416"
          ]
        }
      ],
      "source": [
        "tuner.search([x_train_review_pad,x_train_rating,x_train_category,x_train_word_category], y_train, epochs=5, \n",
        "         validation_data=([x_val_review_pad,x_val_rating,x_val_category,x_val_word_category], y_val),\n",
        "         callbacks=[stop_early,tf.keras.callbacks.TensorBoard(\"/tmp/tb_logs\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-8rLD7EUPlN"
      },
      "outputs": [],
      "source": [
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aWIQnZ1UPlP"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models(num_models=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SplFRjI8UPlP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucqgjp8MUPlQ"
      },
      "outputs": [],
      "source": [
        "# evaluate and save model \n",
        "\n",
        "best_model[0].evaluate([x_test_review_pad, x_test_rating, x_test_category, x_test_word_category], y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLjBo1ntUPlQ"
      },
      "outputs": [],
      "source": [
        "best_model[0].save('/content/drive/MyDrive/swm-data/bilstm_new_tr.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5eOTcIrUPlQ",
        "outputId": "a8d2ad44-aea4-43a6-9a0a-dcc554e9048d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "127/127 - 2s - loss: 0.5196 - acc: 0.7428 - 2s/epoch - 14ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.5196394324302673, 0.742828905582428]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Reload the model and verify the accuracy.\n",
        "\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/swm-data/bilstm_new_tr.h5')\n",
        "model.evaluate([x_test_review_pad, x_test_rating, x_test_category, x_test_word_category], y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0qkJUCv_QxU",
        "outputId": "74f8f1c6-4505-457d-96b5-776e8414ae5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "127/127 [==============================] - 4s 11ms/step\n",
            "Test accuracy: 0.7428288822947576\n",
            "Test precision: 0.7045855379188712\n",
            "Test recall: 0.8119918699186992\n",
            "Test F1 score: 0.7544853635505194\n",
            "Test ROC-AUC score: 0.8300968188511364\n",
            "Test confusion matrix:\n",
            " [[1406  670]\n",
            " [ 370 1598]]\n"
          ]
        }
      ],
      "source": [
        "# Get predictions \n",
        "y_prob = model.predict([x_test_review_pad, x_test_rating, x_test_category, x_test_word_category])\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_metrics = evaluate_model(y_prob, y_test)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Test accuracy:', test_metrics['accuracy'])\n",
        "print('Test precision:', test_metrics['precision'])\n",
        "print('Test recall:', test_metrics['recall'])\n",
        "print('Test F1 score:', test_metrics['f1'])\n",
        "print('Test ROC-AUC score:', test_metrics['roc_auc'])\n",
        "print('Test confusion matrix:\\n', test_metrics['confusion_matrix'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8bMJMmJ9Qd8D",
        "6RYUv70FUeEy",
        "j3B1DAhsUPlH",
        "06eaWzzjT9Fh",
        "Ln7Iqgl3MnxE"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
